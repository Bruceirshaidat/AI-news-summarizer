# -*- coding: utf-8 -*-
"""AI_News_Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HOrQj5AMpCC7-RrAY5aXh8ACWe4P786O
"""

# Install required packages
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install -q transformers datasets gradio accelerate
!pip install -q bitsandbytes  # For memory-efficient training

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from datasets import Dataset, load_dataset
import pandas as pd
import gradio as gr
from transformers import set_seed

# Set seed for reproducibility
set_seed(7)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load smaller, faster model
model_name = "sshleifer/distilbart-cnn-12-6"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Load dataset
dataset = load_dataset("cnn_dailymail", "3.0.0", split="train[:5000]")

def preprocess_data(examples):
    inputs = tokenizer(
        examples["article"],
        truncation=True,
        padding="max_length",
        max_length=512,
        return_tensors="pt"
    )
    targets = tokenizer(
        examples["highlights"],
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )
    targets["input_ids"][targets["input_ids"] == tokenizer.pad_token_id] = -100
    inputs["labels"] = targets["input_ids"]
    return inputs

tokenized_dataset = dataset.map(
    preprocess_data,
    batched=True,
    remove_columns=["article", "highlights", "id"]
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./optimized_summarizer",
    eval_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=400,
    learning_rate=3e-5,  # Slightly lower LR for stability
    per_device_train_batch_size=2,
    per_device_eval_batch_size=4,
    num_train_epochs=1,
    weight_decay=0.01,
    logging_steps=50,
    report_to=None,
    dataloader_pin_memory=False,
    gradient_checkpointing=True,
    warmup_steps=100,
    save_total_limit=2
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset.select(range(500)),
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)
)

trainer.train()
trainer.save_model("./optimized_summarizer_final")
tokenizer.save_pretrained("./optimized_summarizer_final")

# Enhanced inference pipeline with better parameters
summarizer = pipeline(
    "summarization",
    model="./optimized_summarizer_final",
    tokenizer=tokenizer,
    device=0 if device.type == 'cuda' else -1,
    max_length=128,
    min_length=20,
    do_sample=True,  # Enable sampling for more diverse output
    top_p=0.9,
    temperature=0.7,
    repetition_penalty=1.2
)

def summarize_news(text):
    if len(text.strip()) < 50:
        return "Please enter a longer article (at least 50 characters)"

    # Handle long texts by chunking
    if len(text) > 1024:
        text = text[:1024]  # Truncate very long texts

    try:
        result = summarizer(
            text,
            max_length=128,
            min_length=20,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
            repetition_penalty=1.2
        )
        return result[0]['summary_text']
    except Exception as e:
        return f"Error processing text: {str(e)}"

with gr.Blocks(theme=gr.themes.Default(primary_hue="blue", secondary_hue="slate"), title="News Summarizer") as demo:
    gr.Markdown(
        """
        # ðŸ“° AI News Summarizer
        *Transform lengthy articles into concise summaries*
        """
    )

    with gr.Row():
        with gr.Column():
            article_input = gr.TextArea(
                label="Enter Article",
                placeholder="Paste your news article or document here...",
                lines=12,
                value="Google Cloud service integration: Provides AI assistance directly within services like Firebase (app error analysis, performance insights), Colab Enterprise (Python code generation), BigQuery (natural language to SQL, query optimization), Cloud Run, and Apigee"
            )
            submit_btn = gr.Button("Generate Summary", variant="primary", size="lg")

        with gr.Column():
            summary_output = gr.TextArea(
                label="Summary",
                placeholder="Summary will appear here...",
                lines=12,
                interactive=False
            )

    gr.Examples(
        examples=[
            ["Google Cloud service integration: Provides AI assistance directly within services like Firebase (app error analysis, performance insights), Colab Enterprise (Python code generation), BigQuery (natural language to SQL, query optimization), Cloud Run, and Apigee"]
        ],
        inputs=article_input,
        outputs=summary_output,
        label="Example Articles"
    )

    gr.Markdown(
        """
        > **Note:** Works best with news-style content. For technical documents, results may vary.
        """
    )

    submit_btn.click(
        fn=summarize_news,
        inputs=article_input,
        outputs=summary_output
    )

demo.launch(share=True)

